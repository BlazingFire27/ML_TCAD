{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14621929,"sourceType":"datasetVersion","datasetId":9293450}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom pytorch_lightning.callbacks import EarlyStopping\n\nfrom pina import Condition, Span, LabelTensor, Trainer\nfrom pina.problem import TimeDependentProblem, SpatialProblem, ParametricProblem\nfrom pina.operators import grad\nfrom pina.model import FeedForward\nfrom pina.solver import PINN\n\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_filepath = \"/kaggle/input/cleaned-oxi3\"\nfile_paths = [os.path.join(input_filepath, fname) for fname in os.listdir(input_filepath)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class OxidationDataLoader(Dataset):\n    def __init__(self, file_paths):\n        # I am creating a list of all dfs\n        dfs = []\n        for f in file_paths:\n            df = pd.read_csv(f)\n            dfs.append(df)\n\n        self.df = pd.concat(dfs, ignore_index = True)\n        # I am converting everything to numbers from strings like 1e15 etc\n        self.df['X'] = pd.to_numeric(self.df['X'], errors = 'coerce')\n        self.df['Y'] = pd.to_numeric(self.df['Y'], errors = 'coerce')\n        self.df['Time'] = pd.to_numeric(self.df['Time'], errors='coerce')\n        self.df['O2 Flow'] = pd.to_numeric(self.df['O2 Flow'], errors='coerce')\n        self.df['N2 Flow'] = pd.to_numeric(self.df['N2 Flow'], errors='coerce')\n        self.df['Temperature'] = pd.to_numeric(self.df['Temperature'], errors='coerce')\n\n        self.df.dropna(subset = ['X', 'Y', 'Time', 'O2 Flow', 'N2 Flow', 'Temperature'], inplace = True)\n\n        epsilon = 1e-9 # to avoid -infi\n        self.df.loc[self.df['Y'] <= 0, 'Y'] = epsilon\n\n        # I am converting Y outputs to log Y because only 4% of total dataset\n        # has oxidation conc values around 96% has only bulk pure Si\n        # to in order to not let the model simply memorise bulk we do\n        self.df['logY'] = np.log10(self.df['Y'])\n\n        # logY > 2 --> Y > 100\n        mask_reactive = self.df['logY'] > 2.0\n        # two different dfs for reacted part and bulk part\n        df_reactive = self.df[mask_reactive]\n        df_bulk = self.df[~mask_reactive]\n\n        df_bulk = df_bulk.iloc[:10]        # only 10 data points after si sio2 interface\n        # n_reactive = len(df_reactive)\n        # if len(df_bulk) > n_reactive:\n        #     df_bulk = df_bulk.sample(n = n_reactive, random_state = 0)\n\n        self.df_balanced = pd.concat([df_reactive, df_bulk]).reset_index(drop = True)\n        self.df_balanced = self.df_balanced.sample(frac = 1, random_state = 0).reset_index(drop = True)\n        # we are dropping more parts of bulk so that now\n        # the oxidation concentrated part is not minority and mmodel doesnt overfit\n\n        features = self.df_balanced[['X', 'Time', 'O2 Flow', 'N2 Flow', 'Temperature']].values\n        targets = self.df_balanced['logY'].values\n\n        self.inputs = torch.tensor(features, dtype = torch.float32)\n        self.targets = torch.tensor(targets, dtype = torch.float32).unsqueeze(-1)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class OxidationPhysics(TimeDependentProblem, SpatialProblem, ParametricProblem):\n    def __init__(self, data_loader):\n        super().__init__()\n\n        df = data_loader.df_balanced\n\n        x_min, x_max = df['X'].min(), df['X'].max()\n        self.spatial_domain = Span({'x': [x_min, x_max]})\n\n        t_min, t_max = df['Time'].min(), df['Time'].max()\n        self.temporal_domain = Span({'t': [t_min, t_max]})\n\n        self.params_domain = Span({\n            'o2': [df['O2 Flow'].min(), df['O2 Flow'].max()],\n            'n2': [df['N2 Flow'].min(), df['N2 Flow'].max()],\n            'temp': [df['Temperature'].min(), df['Temperature'].max()]\n        })\n\n        labeled_inputs = LabelTensor(\n            data_loader.inputs,\n            ['x', 't', 'o2', 'n2', 'temp']\n        )\n\n        labeled_targets = LabelTensor(\n            data_loader.targets,\n            ['u']\n        )\n\n        self.conditions = {\n            'data': Condition(\n                input_points = labeled_inputs,\n                output_points = labeled_targets\n            ),\n\n            'physics': Condition(\n                equation = self.diffusion_equation,\n\n                location = Span({\n                    'x': [x_min, x_max], \n                    't': [t_min, t_max], \n                    'o2': [df['O2 Flow'].min(), df['O2 Flow'].max()], \n                    'n2': [df['N2 Flow'].min(), df['N2 Flow'].max()], \n                    'temp': [df['Temperature'].min(), df['Temperature'].max()]\n                })\n            ),\n\n            'surface': Condition(\n                equation = self.surface_bc,\n\n                location = Span({\n                    'x': [0, 1e-6], # Very close to surface\n                    't': [t_min, t_max],\n                    'o2': [df['O2 Flow'].min(), df['O2 Flow'].max()],\n                    'n2': [df['N2 Flow'].min(), df['N2 Flow'].max()],\n                    'temp': [df['Temperature'].min(), df['Temperature'].max()]\n                })\n            ),\n        }\n\n    def diffusion_equation(self, input_, output_):\n        \"\"\"\n        The Physics: u_t - D * u_xx = 0\n        \"\"\"\n\n        u_t = grad(output_, input_, d = 't')\n        u_xx = grad(output_, input_, d = 'x', dd = 'x')\n\n        # Physics Logic: Diffusivity D depends on Temperature\n        # D ~ Temperature / 1000 (Scaled approximate)\n        \n        temp = input_.extract(['temp'])\n        D = 1e-4 * (temp/1000.0)\n\n        return u_t - (D * u_xx)\n\n    def surface_bc(self, input_, output_):\n        \"\"\"\n        The Boundary: u(surface) approx 14 * O2_Flow\n        \"\"\"\n\n        u = output_.extract(['u'])\n        o2 = input_.extract(['o2'])\n\n        return u - (14.0 * o2) # 14.0 must actually be a trainable param\n# this is approximation for partial pressure we will include henry's law here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(input_dim: int, output_dim: int,\n                 n_layers: int, n_neurons: int, activation: str = 'tanh') -> torch.nn.Module:\n\n    layers = []\n\n    layers.append({\n        'input_dim': input_dim,\n        'output_dim': n_neurons,\n        'activation': activation\n    })\n\n    for _ in range(n_layers - 1):\n        layers.append({\n            'input_dim': n_neurons,\n            'output_dim': n_neurons,\n            'activation': activation\n        })\n\n    layers.append({\n        'input_dim': n_neurons,\n        'output_dim': output_dim,\n        'activation': None\n    })\n\n    model = FeedForward(\n        layers = layers,\n        input_variables = ['x', 't', 'o2', 'n2', 'temp'],\n        output_variables = ['u']\n    )\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    n_layers = trial.suggest_int('n_layers', 3, 6)\n    n_neurons = trial.suggest_int('n_neurons', 32, 128)\n    activation = trial.suggest_categorical('activation', ['tanh', 'sigmoid'])\n\n    lr = trial.suggest_float('learning_rate', 1e-4, 1e-2, log = True)\n\n    w_data = trial.suggest_float('weight_data', 1.0, 20.0)\n    w_phys = trial.suggest_float('weight_phys', 0.1, 5.0)\n    w_surf = trial.suggest_float('weight_surf', 1.0, 10.0)\n\n    problem = OxidationPhysics(data_loader = loader)\n\n    problem.conditions['data'].weight = w_data\n    problem.conditions['physics'].weight = w_phys\n    problem.conditions['surface'].weight = w_surf\n\n    problem.discretise_domain(\n        n = 2000,\n        mode = 'random',\n        locations = ['physics', 'surface']\n    )\n\n    model = create_model(\n        input_dim=5, \n        output_dim=1, \n        n_layers=n_layers, \n        n_neurons=n_neurons, \n        activation=activation\n    )\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    solver = PINN(\n        problem=problem,\n        model=model,\n        optimizer=optimizer\n    )\n\n    trainer = Trainer(\n        solver=solver,\n        max_epochs=40,\n        accelerator='auto',\n        logger=False,\n        enable_progress_bar=False,\n        enable_model_summary=False\n    )\n\n    trainer.train()\n\n    final_loss = trainer.logged_metrics.get('mean_loss', None)\n\n    if final_loss is None:\n        final_loss = trainer.logged_metrics.get('loss', 1e9)\n        \n    return final_loss.item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_best_model(best_params, data_loader, epochs = 500, output_dir = 'results'):\n    print(f\"Starting final training for {epochs} epochs\")\n\n    problem = OxidationPhysics(data_loader)\n\n    problem.conditions['data'].weight = best_params.get('weight_data', 10.0)\n    problem.conditions['physics'].weight = best_params.get('weight_phys', 1.0)\n    problem.conditions['surface'].weight = best_params.get('weight_surf', 5.0)\n\n    problem.discretise_domain(\n        n=4000, \n        mode='random', \n        locations=['physics', 'surface']\n    )\n\n    model = create_model(\n        input_dim=5, \n        output_dim=1, \n        n_layers=best_params['n_layers'], \n        n_neurons=best_params['n_neurons'], \n        activation=best_params.get('activation', 'tanh')\n    )\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), \n        lr=best_params['learning_rate']\n    )\n\n    solver = PINN(\n        problem=problem,\n        model=model,\n        optimizer=optimizer\n    )\n\n    early_stop_callback = EarlyStopping(\n        monitor = \"mean_loss\",\n        min_delta = 1e-4,\n        patience = 50,\n        verbose = True,\n        mode = \"min\"\n    )\n\n    trainer = Trainer(\n        solver=solver,\n        max_epochs=epochs,\n        accelerator='auto',\n        default_root_dir=output_dir,\n        enable_progress_bar=True,\n        enable_model_summary=True,\n        callbacks = [early_stop_callback]\n    )\n\n    trainer.train()\n    print(\"Training Complete\")\n\n    final_train_loss = trainer.logged_metrics.get('mean_loss', torch.tensor(-1.0)).item()\n    print(f\"Final Training Loss: {final_train_loss:.6f}\")\n\n    os.makedirs(output_dir, exist_ok = True)\n    save_path = os.path.join(output_dir, \"best_model.pth\")\n    torch.save(solver.model.state_dict(), save_path)\n    print(f\"Model saved to {save_path}\")\n\n    return solver, trainer, final_train_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_performance(solver, data_loader, train_loss = None):\n    print(\"Evaluating model performance\")\n\n    device = solver.device if hasattr(solver, 'device') else 'cpu'\n\n    inputs = data_loader.inputs.to(device)\n    true_log_u = data_loader.targets.cpu().numpy()\n    \n    solver.model.eval()\n    with torch.no_grad():\n        preds_log_u = solver.model(inputs).cpu().numpy()\n\n    mask_react = true_log_u > 2.0\n    mse_log_react = mean_squared_error(true_log_u[mask_react], preds_log_u[mask_react])\n    rmse_log_react = np.sqrt(mse_log_react)\n\n    mse_log_bulk = mean_squared_error(true_log_u[~mask_react], preds_log_u[~mask_react])\n    rmse_log_bulk = np.sqrt(mse_log_bulk)\n    \n    true_Y = np.power(10, true_log_u)\n    pred_Y = np.power(10, preds_log_u)\n\n    relative_error = np.abs((true_Y - pred_Y) / (true_Y + 1e-9))\n    median_rel_error = np.median(relative_error) * 100\n    mean_rel_error = np.mean(relative_error) * 100\n\n    print(f\"   Reaction Zone RMSE (Log): {rmse_log_react:.4f}\")\n    print(f\"   Bulk Zone RMSE (Log):     {rmse_log_bulk:.4f}\")\n    print(f\"   Median Rel Error (Y):     {median_rel_error:.2f}%\")\n\n    if train_loss is not None:\n        total_rmse = np.sqrt(mean_squared_error(true_log_u, preds_log_u))\n        print(f\"   Gap (Eval RMSE - Train Loss): {total_rmse - train_loss:.4f}\")\n    \n    return rmse_log_react, rmse_log_bulk, median_rel_error","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}